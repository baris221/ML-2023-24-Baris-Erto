{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TE2ItlsI956"
   },
   "source": [
    "# ML - TME 3 : Introduction à Pytorch\n",
    "\n",
    "\n",
    "Nicolas Baskiotis (nicolas.baskiotis@sorbonne-universite.fr)  -- MLIA/ISIR, Sorbonne Université\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAER7frwJu9L"
   },
   "source": [
    "# Préambule\n",
    "\n",
    "Dans ce TME, vous allez principalement refaire ce que vous avez fait en TME2 mais sous PyTorch, et en remplaçant au fur et à mesure les blocs programmés à la main par les possibilités de PyTorch.\n",
    "\n",
    "Les lignes suivantes permettent d'importer pytorch et vérifier qu'un GPU est disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1700496819634,
     "user": {
      "displayName": "Lau re",
      "userId": "03302099944040145915"
     },
     "user_tz": -60
    },
    "id": "3Y9YOOHHhJKY",
    "outputId": "e7a2d50b-e003-4b81-ba6a-cddab0bfbc3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La version de torch est :  2.2.0+cpu\n",
      "Le calcul GPU est disponible ?  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"La version de torch est : \",torch.__version__)\n",
    "print(\"Le calcul GPU est disponible ? \", torch.cuda.is_available())\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WA5ZFwjkWKS"
   },
   "source": [
    "# Prise en main de Pytorch\n",
    "\n",
    "Cette partie est un tutoriel pour la prise en main de pytorch. N'hesitez pas à explorer et expérimenter les notions présentées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2LqFo3wzwYP"
   },
   "source": [
    "## Syntaxe\n",
    "\n",
    "Le principal objet manipulé sous Pytorch est **torch.Tensor** qui correspond à un tenseur mathématique (généralisation de la notion de matrice en $n$-dimensions), très proche dans l'utilisation de **numpy.array**.   Cet objet est optimisé pour les calculs sur GPU ce qui implique quelques contraintes plus importantes que sous **numpy**. En particulier :\n",
    "* le type du tenseur manipulé est très important et les conversions ne sont pas automatique (**FloatTensor** de type **torch.float**, **DoubleTensor** de type **torch.double**,  **ByteTensor** de type **torch.byte**, **IntTensor** de type **torch.int**, **LongTensor** de type **torch.long**). Pour un tenseur **t** La conversion se fait très simplement en utilisant les fonctions : **t.double()**, **t.float()**, **t.long()** ...\n",
    "* la plupart des opérations ont une version *inplace*, c'est-à-dire qui modifie le tenseur plutôt que de renvoyer un nouveau tenseur; elles sont suffixées par **_** (**add_** par exemple).\n",
    "\n",
    "N'hésitez pas à vous référez à la [documentation officielle](https://pytorch.org/docs/stable/tensors.html) pour la liste exhaustive des opérations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les lignes suivantes ont pour objectif à vous faire prendre en main les tenseurs. Completez afin d'effectuer les opérations demandées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1700496820051,
     "user": {
      "displayName": "Lau re",
      "userId": "03302099944040145915"
     },
     "user_tz": -60
    },
    "id": "VZxNfy1b1u43",
    "outputId": "e1cc6a39-4820-4353-f596-1fbe813ee239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2640,  0.3536,  0.4529, -1.7906, -1.7732,  0.3259],\n",
      "        [ 0.5435, -0.8124, -0.2146,  0.6150,  0.4059, -3.9854]])\n"
     ]
    }
   ],
   "source": [
    "# Création de tenseurs et caractéristiques\n",
    "liste_1, liste_2 = [1.,2.,3.], [2.,3.,4.]\n",
    "## Créez un tenseur à partir des listes liste_1 et liste_2\n",
    "tensor_1=torch.tensor([liste_1,liste_2])\n",
    "#print(tensor_1)\n",
    "\n",
    "## Créez un tenseur  tenseur rempli de 1 de taille 2x3x4\n",
    "tensor_2=torch.ones([2,3,4])\n",
    "#print(tensor_2)\n",
    "\n",
    "##  Créez tenseur de zéros de taille 2x3 de type float\n",
    "tensor_3=torch.zeros([2,3],dtype=float)\n",
    "#print(tensor_3)\n",
    "## Créez un tenseur de zéros de taille 2x3 puis remplissez le d'entiers aléatoires entre 10 et 15 (inplace)\n",
    "tensor_4=torch.zeros([2,3])\n",
    "tensor_4.random_(10,16)\n",
    "#print(tensor_4)\n",
    "\n",
    "##  Créez un tenseur a de taille 2x3 aléatoire suivant une loi normale, et un tenseur b de taille 3x4,\n",
    "#et un vecteur c de taille 3\n",
    "tensor_5_a=torch.normal(0,1,size=(2,3))\n",
    "#print(tensor_5_a)\n",
    "tensor_5_b=torch.normal(0,1,size=(3,4))\n",
    "#print(tensor_5_b)\n",
    "tensor_5_c=torch.normal(0,1,size=(3,))\n",
    "#print(tensor_5_c)\n",
    "##  Concatenez les deux tenseurs a de façons à former un  tenseur de taille 6x2\n",
    "concatenated_tensor = torch.cat((tensor_5_a.t(), tensor_5_a.t()))\n",
    "#print(concatenated_tensor)\n",
    "\n",
    "##  Affichez le nombre de lignes, de colonnes  de b\n",
    "#print(concatenated_tensor.shape)\n",
    "\n",
    "## Convertissez a en tenseur d'entiers\n",
    "\n",
    "tensor_a_int=tensor_5_a.to(torch.int)\n",
    "#print(tensor_a_int)\n",
    "# Opérations élémentaires sur les tenseurs\n",
    "##Faites le produit scalaire de c par c\n",
    "tensor_op1=torch.dot(tensor_5_c,tensor_5_c)\n",
    "#print(tensor_op1)\n",
    "\n",
    "## Faites le produit matriciel de a par b\n",
    "tensor_op2=torch.matmul(tensor_5_a,tensor_5_b)\n",
    "#print(tensor_op2)\n",
    "\n",
    "## Calculez le produit matriciel de la transposé de a par a\n",
    "tensor_op3=torch.matmul(tensor_5_a,tensor_5_a.t())\n",
    "#print(tensor_op3)\n",
    "\n",
    "## Trouvez l'index des maximums des colonnes de a\n",
    "tensor_5_a=torch.normal(0,1,size=(2,3))\n",
    "#print(tensor_5_a)\n",
    "#print(torch.argmax(tensor_5_a,axis=0))\n",
    "\n",
    "\n",
    "## Faites la somme par colonne de b, la somme totale de b\n",
    "tensor_5_b=torch.normal(0,1,size=(3,4))\n",
    "#print(tensor_5_b)\n",
    "#print(torch.sum(tensor_5_b,axis=0))\n",
    "#print(torch.sum(tensor_5_b))\n",
    "\n",
    "## Calculez la moyenne selon les colonnes de b, puis la moyenne de tous les éléments\n",
    "tensor_5_b=torch.normal(0,1,size=(3,4))\n",
    "#print(tensor_5_b)\n",
    "#print(torch.mean(tensor_5_b,axis=0))\n",
    "#print(torch.mean(tensor_5_b))\n",
    "\n",
    "## Changez la taille de b en 2x6\n",
    "tensor_5_b=torch.normal(0,1,size=(3,4))\n",
    "tensor_5_b_reshaped=tensor_5_b.reshape(2,6)\n",
    "print(tensor_5_b_reshaped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention ! comme sous numpy, il peut y avoir des pièges !\n",
    "Vérifier toujours les dimensions !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) tensor([[0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=torch.zeros(5,1)\n",
    "b=torch.zeros(5)\n",
    "## la première opération fait un broadcast et le résultat est tenseur à 2 dimensiosn,\n",
    "## le résultat de la deuxième opération est bien un vecteur\n",
    "print(a-b,a.t()-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzhX7D8KKIvt"
   },
   "source": [
    "## Autograd et graphe de calcul\n",
    "Un élément central de pytorch est le graphe de calcul : lors du calcul d'une variable, l'ensemble des opérations qui ont servies au calcul sont stockées sous la forme d'un graphe acyclique, dit de *calcul*. Les noeuds internes du graphe représentent les opérations, le noeud terminal le résultat et les racines les variables d'entrées. Ce graphe sert en particulier à calculer les dérivées partielles de la sortie par rapport aux variables d'entrées - en utilisant les règles de dérivations chainées des fonctions composées.\n",
    "Pour cela, toutes les fonctions disponibles dans pytorch comportent un mécanisme, appelé *autograd* (automatique differentiation), qui permet de calculer les dérivées partielles des opérations.\n",
    "\n",
    "### Différenciation automatique\n",
    "(De manière simplifiée, pour les détails cf [la documentation](https://pytorch.org/docs/stable/notes/extending.html))\n",
    "\n",
    "Toute opération sous pytorch hérite de la classe **Function** et doit définir :\n",
    "* une méthode **forward(\\*args)** : passe avant, calcule le résultat de la fonction appliquée aux arguments\n",
    "* une méthode **backward(\\*args)** : passe arrière, calcule les dérivées partielles par rapport aux entrées. Les arguments de  cette méthode correspondent aux valeurs des dérivées suivantes dans le graphe de calcul. En particulier, il y a autant d'arguments à **backward**  que de sorties pour la méthode **forward** (rétro-propagation : on doit connaître les dérivés qui viennent  en aval du calcul) et autant de sorties que d'arguments dans la méthode **forward** (chaque sortie correspond à  une dérivée partielle par rapport à chaque entrée du module). Le calcul se fait sur les valeurs du dernier appel de **forward**.\n",
    "\n",
    "Par exemple, pour la fonction d'addition  **add(x,y)**, **add.forward(x,y)** renverra **x+y** (l'appel de la fonction est équivalent à l'appel de **forward**) et **add.backward(1)** renverra le couple **(1,1)** (la dérivée par rapport à x, et celle par rapport à y) .\n",
    "\n",
    "\n",
    "\n",
    "En pratique, ce ne sont pas les méthodes de ces fonctions qui sont utilisées, mais des méthodes équivalentes sur les tenseurs. La méthode **backward** d'un tenseur permet de rétro-propager le calcul du gradient sur toutes les variables qui ont servies à son calcul.\n",
    "\n",
    "La valeur du gradient pour chaque dérivée partielle se trouve dans l'attribut **grad** de la variable concernée.\n",
    "\n",
    "Comme c'est un mécanisme lourd, l'autograd n'est pas activé par défaut pour une variable. Afin de l'activer, il faut mettre le flag **requires_grad** de cette variable à vrai. Dès lors, tout calcul qui utilise cette variable sera enregistré dans le graphe de calcul et le gradient sera disponible.\n",
    "\n",
    "\n",
    "Exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700496820051,
     "user": {
      "displayName": "Lau re",
      "userId": "03302099944040145915"
     },
     "user_tz": -60
    },
    "id": "J_mYVeXMfsTV",
    "outputId": "2e6001f5-b1ed-4d8d-ba8b-e6fec033aa84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe de calcul ?  False\n",
      "Dérivée de z/a :  2.0  z/b : 1.0\n",
      "Erreur :  element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1.)\n",
    "# Par défaut, requires_grad est à False\n",
    "print(\"Graphe de calcul ? \",a.requires_grad)\n",
    "# On peut demander à ce que le graphe de calcul soit retenu\n",
    "a.requires_grad = True\n",
    "# Ou lors de la création du tenseur directement\n",
    "b = torch.tensor(2.,requires_grad=True)\n",
    "z = 2*a + b\n",
    "# Calcul des dérivées partielles par rapport à z\n",
    "z.backward()\n",
    "print(\"Dérivée de z/a : \", a.grad.item(),\" z/b :\", b.grad.item())\n",
    "\n",
    "# Si on a oublié de demander le graphe de calcul :\n",
    "a, b = torch.tensor(1.),torch.tensor(2.)\n",
    "z = 2*a+b\n",
    "try:\n",
    "  z.backward()\n",
    "except Exception as e:\n",
    "  print(\"Erreur : \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eApYmHIZa917"
   },
   "source": [
    "### <span class=\"alert-success\">     Exercice :  Utilisation de backward     </div>\n",
    "* Implémentez (en une ligne) la fonction de coût aux moindres carrés $MSE(\\hat{y},y)=\\frac{1}{2N} \\sum_{i=1}^N\\|\\hat{y_i}-y_i\\|^2$ où $\\hat{y},y$ sont deux matrices de taille $N\\times d$, et $y_i,\\hat{y_i}$ les $i$-èmes vecteurs lignes des matrices.\n",
    "\n",
    "Vérifiez bien en début de code que $\\hat{y}$ et $y$ sont des matrices et non pas des vecteurs, pour ne pas avoir d'erreur lors du broadcasting.\n",
    "\n",
    "* Engendrez **y,yhat** deux matrices aléatoires de taille $(1,5)$.\n",
    "* Calculez **MSE(y,yhat)** et l'afficher\n",
    "* Calculez à la main le gradient de **MSE** par rapport à **y**, **yhat**\n",
    "* Calculez grâce à pytorch le gradient de **MSE** par rapport à **y** et **yhat** et vérifier le résultat. Observez la dimension de sortie, est-elle bien compatible avec la taille de **y** et **yhat** ?\n",
    "* Appelez une deuxième fois **MSE** sur les mêmes vecteurs et la méthode **backward**. Qu'observez vous pour le gradient ? Comment l'expliquez vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "4Rd37M_3gkqu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9173, 0.3918, 0.7769, 0.8702, 0.2578]])\n",
      "tensor([[0.3206, 0.4586, 0.0160, 0.3811, 0.0897]])\n",
      "tensor(0.6035, grad_fn=<MulBackward0>)\n",
      "Y derive partielle [[0.5967945456504822, -0.06685256958007812, 0.7608470916748047, 0.4890565276145935, 0.16809649765491486]]\n",
      "Y_hat derivee partielle [[-0.5967945456504822, 0.06685256958007812, -0.7608470916748047, -0.4890565276145935, -0.16809649765491486]]\n"
     ]
    }
   ],
   "source": [
    "def MSE(y,y_hat):\n",
    "    #print(y.size())\n",
    "    return 1/(2*len(y))*torch.linalg.norm(y-y_hat)**2\n",
    "\n",
    "y=torch.rand(1,5)\n",
    "print(y)\n",
    "y_hat=torch.rand(1,5)\n",
    "print(y_hat)\n",
    "y.requires_grad=True\n",
    "y_hat.requires_grad=True\n",
    "mse=MSE(y,y_hat)\n",
    "print(mse)\n",
    "mse.backward()\n",
    "print(\"Y derive partielle\",y.grad.tolist())\n",
    "print(\"Y_hat derivee partielle\",y_hat.grad.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivé partielle de Y est égal à l'inverse de Y_hat.La valeur de dérive est la diff entre  Y et Y_hat, donc on peut utiliser cette fonction pour obtenir une approximation de la dérivée partielle de Y en utilisant les données de Y et Y_hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxgCbApIgU5X"
   },
   "source": [
    "\n",
    "### <span class=\"alert-success\"> Exercice :   Régression linéaire en pytorch </span>\n",
    "\n",
    "* Définissez la fonction **flineaire(x,w,b)** fonction linéaire qui calcule $f(x,w,b)=x.w^t+b$  avec $x\\in \\mathbb{R}^{{n\\times d}},~w\\in\\mathbb{R}^{1,d}, b\\in \\mathbb{R}$\n",
    "* Ecrivez le code de **descente_gradient(datax,datay,loss, eps, epochs)** pour réaliser une descente de gradient sur les données **datax, datay** avec le coût **loss**, un pas de gradient **eps** et un nombre d'itérations **epochs**. Elle doit renvoyer les paramètre **w** et **b** optimaux : $$w^∗,b^∗=argmin_{w,b}\\frac{1}{N} \\sum_{i=1}^N loss(f(x^i,w,b),y^i)$$\n",
    "\n",
    "Pour tester votre code, utilisez soit les données simulées du TME2, soit le jeu de données très classique *Boston*, le prix des loyers à Boston en fonction de caractéristiques socio-économiques des quartiers. Le code ci-dessous permet de les charger.\n",
    "\n",
    "* Visualisez la courbe d'erreur en fonction du nombre d'itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples :  20640 Dimension :  8\n",
      "Nom des attributs :  MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
      "tensor([[ 8.3252e+00,  4.1000e+01,  6.9841e+00,  1.0238e+00,  3.2200e+02,\n",
      "          2.5556e+00,  3.7880e+01, -1.2223e+02],\n",
      "        [ 8.3014e+00,  2.1000e+01,  6.2381e+00,  9.7188e-01,  2.4010e+03,\n",
      "          2.1098e+00,  3.7860e+01, -1.2222e+02],\n",
      "        [ 7.2574e+00,  5.2000e+01,  8.2881e+00,  1.0734e+00,  4.9600e+02,\n",
      "          2.8023e+00,  3.7850e+01, -1.2224e+02],\n",
      "        [ 5.6431e+00,  5.2000e+01,  5.8174e+00,  1.0731e+00,  5.5800e+02,\n",
      "          2.5479e+00,  3.7850e+01, -1.2225e+02],\n",
      "        [ 3.8462e+00,  5.2000e+01,  6.2819e+00,  1.0811e+00,  5.6500e+02,\n",
      "          2.1815e+00,  3.7850e+01, -1.2225e+02]])\n",
      "Nombre d'exemples :  20640 Dimension :  8\n",
      "Nom des attributs :  MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
      "tensor(1.) tensor(-1.) tensor(1.) tensor(-1.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Chargement des données Boston et transformation en tensor.\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing(data_home=\"./data/\") ## chargement des données\n",
    "housing_x = torch.tensor(housing['data'],dtype=torch.float) # penser à typer les données pour éliminer les incertitudes\n",
    "housing_y = torch.tensor(housing['target'],dtype=torch.float)\n",
    "print(\"Nombre d'exemples : \",housing_x.size(0), \"Dimension : \",housing_x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(housing['feature_names']))\n",
    "\n",
    "print(housing_x[:5])\n",
    "\n",
    "print(\"Nombre d'exemples : \",housing_x.size(0), \"Dimension : \",housing_x.size(1))\n",
    "print(\"Nom des attributs : \", \", \".join(housing['feature_names']))\n",
    "\n",
    "# On centre norme les données, sinon... Testez pour voir !\n",
    "housing_x = 2*(housing_x-housing_x.min(0)[0])/(housing_x.max(0)[0]-housing_x.min(0)[0])-1\n",
    "housing_y = 2*(housing_y-housing_y.min(0)[0])/(housing_y.max(0)[0]-housing_y.min(0)[0])-1\n",
    "print(housing_x.max(),housing_x.min(),housing_y.max(),housing_y.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2149,
     "status": "ok",
     "timestamp": 1700496822197,
     "user": {
      "displayName": "Lau re",
      "userId": "03302099944040145915"
     },
     "user_tz": -60
    },
    "id": "dArOgSWNTVvb",
    "outputId": "3c737f4c-2336-4f58-ab16-31c6b8e53f35"
   },
   "outputs": [],
   "source": [
    "def flineaire(x,w,b):\n",
    "    return torch.matmul(x,w.t())+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n"
     ]
    }
   ],
   "source": [
    "#print(len(housing_y))\n",
    "def descente_gradient(datax,datay,loss,eps,epoch):\n",
    "    w = torch.randn(1, datax.shape[1])\n",
    "    w.requires_grad=True\n",
    "    b = torch.randn(1)\n",
    "    b.requires_grad=True\n",
    "    y_hat=flineaire(datax,w,b)\n",
    "    taux_erreur=loss(datay,y_hat)\n",
    "    taux_erreur.backward()\n",
    "    liste_erreur=[]\n",
    "\n",
    "    for i in range(epoch):\n",
    "        #print(i)\n",
    "        y_hat=flineaire(datax,w,b)\n",
    "        taux_erreur=loss(datay,y_hat)\n",
    "        #print(taux_erreur)\n",
    "        #print(w.grad)\n",
    "        with torch.no_grad():\n",
    "            dw = eps * w.grad.detach()\n",
    "            db = eps * b.grad.detach()\n",
    "            w -= dw\n",
    "            b -= db\n",
    "        liste_erreur.append(taux_erreur.item())\n",
    "\n",
    "    return w,b,liste_erreur\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opti,b_opti,liste_erreur=descente_gradient(housing_x,housing_y,MSE,0.1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1700496822595,
     "user": {
      "displayName": "Lau re",
      "userId": "03302099944040145915"
     },
     "user_tz": -60
    },
    "id": "sLKQ7Z1dECls",
    "outputId": "4f7b28b6-721c-42af-e8af-ea823a9c073c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8392.1181640625, 22914283520.0, 91690336256.0, 218197639168.0, 366781333504.0, 567753768960.0, 872790556672.0, 1117872193536.0, 1467166752768.0, 1850776092672.0, 2271727321088.0, 2767078293504.0, 3491162226688.0, 3979715018752.0, 4471488774144.0, 5081374654464.0, 5869025624064.0, 6982324977664.0, 7405953351680.0, 8398995718144.0, 9088782041088.0, 10003292356608.0, 11069111140352.0, 12291889168384.0, 13964648906752.0, 14050549301248.0, 15918860075008.0, 16910997520384.0, 17885955096576.0, 19126948986880.0, 20325498617856.0, 21890838036480.0, 23477490810880.0, 25947608711168.0, 27929299910656.0, 27963655454720.0, 29630111154176.0, 32138363666432.0, 33598553980928.0, 34827013193728.0, 36356021551104.0, 38220045746176.0, 40015337881600.0, 41990397886464.0, 44276553613312.0, 46335453560832.0, 49171264438272.0, 53298564431872.0, 55858595627008.0, 55875771301888.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(liste_erreur)\n",
    "\n",
    "def plot_loss(list_erreur):\n",
    "    list_i=[i for i in range(len(list_erreur))]\n",
    "    plt.plot(list_i, list_erreur)\n",
    "    plt.xlabel(\"Le nombre d'iter\")\n",
    "    plt.ylabel(\"Le taux d'erreur\")\n",
    "    plt.title(\"Taux d'erreur en fonction de nombre d'iter\")\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(liste_erreur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRwqLJPRIpXu"
   },
   "source": [
    "\n",
    "## Optimiseur\n",
    "Pytorch inclut une classe très utile pour la descente de gradient, [torch.optim](https://pytorch.org/docs/stable/optim.html), qui permet :\n",
    "* d'économiser quelques lignes de codes\n",
    "* d'automatiser la mise-à-jour des paramètres\n",
    "* d'abstraire le type de descente de gradient utilisé (sgd,adam, rmsprop, ...)\n",
    "\n",
    "Une liste de paramètres à optimiser est passée à l'optimiseur lors de l'initialisation. La méthode **zero_grad()** permet de remettre le gradient à zéro et la méthode **step()** permet de faire une mise-à-jour des paramètres.\n",
    "\n",
    "Un exemple de code  utilisant l'optimiseur est donné ci-dessous. \n",
    "\n",
    "Ré-implémentez la descente de gradient en utilisant un optimiseur. \n",
    "\n",
    "L'optimiseur le plus utilisé  est **Adam** qui permet une pas adaptatif en fonction de l'historique. Sur ce problème simple vous ne verrez pas de différence, vous pourrez comparer par la suite **SGD** et **Adam**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W init :  tensor([ 0.7131, -0.7310, -0.2511, -0.8285,  0.1622,  0.3894, -0.2038, -0.5364,\n",
      "        -0.3664,  0.5643,  0.1955,  0.8044,  0.0686,  1.7690, -2.2179,  0.7077,\n",
      "        -0.4846,  1.7863, -0.2217,  1.3794,  0.0791,  0.6881,  0.9746, -0.3042,\n",
      "        -0.5262,  1.4498,  0.4453,  0.8707, -0.7601, -0.8082,  0.4665, -2.3467,\n",
      "        -0.5376,  1.8054,  2.2813,  0.1680,  1.5347, -0.4939,  0.5980,  0.2086,\n",
      "         0.7688, -2.1290, -0.7039, -0.1676,  0.6199, -0.4975,  1.1168,  2.4291,\n",
      "         1.7000,  1.9461, -0.1466, -0.6167, -0.2259,  1.0198, -0.5967, -1.3818,\n",
      "         0.6170, -1.3896,  1.7213, -1.4848, -0.6846,  0.6466,  0.0954, -0.7323,\n",
      "         0.5091,  0.6113, -0.6161,  0.3261,  1.0273,  0.9064, -0.0560,  0.8863,\n",
      "        -0.1711,  1.2118,  0.5055, -0.4588, -1.4233,  0.4671,  0.3785,  0.6382,\n",
      "        -1.1068, -1.9888, -0.3325,  0.4038,  0.9196,  1.6531, -0.8145,  0.1160,\n",
      "         0.5658,  0.6786,  0.1483, -1.9722,  1.8924,  0.8527, -1.6467,  0.4915,\n",
      "        -1.1012,  1.0419, -0.1373, -0.3748], requires_grad=True)\n",
      "gradient :  tensor([ 1.4262, -1.4621, -0.5022, -1.6571,  0.3244,  0.7787, -0.4076, -1.0728,\n",
      "        -0.7327,  1.1286,  0.3911,  1.6088,  0.1371,  3.5381, -4.4359,  1.4154,\n",
      "        -0.9691,  3.5725, -0.4433,  2.7589,  0.1583,  1.3762,  1.9491, -0.6083,\n",
      "        -1.0524,  2.8996,  0.8906,  1.7415, -1.5202, -1.6164,  0.9329, -4.6934,\n",
      "        -1.0752,  3.6107,  4.5627,  0.3361,  3.0694, -0.9877,  1.1960,  0.4171,\n",
      "         1.5375, -4.2579, -1.4077, -0.3353,  1.2397, -0.9951,  2.2336,  4.8583,\n",
      "         3.3999,  3.8923, -0.2933, -1.2335, -0.4519,  2.0396, -1.1935, -2.7637,\n",
      "         1.2340, -2.7792,  3.4425, -2.9697, -1.3692,  1.2931,  0.1907, -1.4645,\n",
      "         1.0182,  1.2225, -1.2322,  0.6522,  2.0546,  1.8128, -0.1120,  1.7726,\n",
      "        -0.3423,  2.4236,  1.0111, -0.9175, -2.8466,  0.9343,  0.7570,  1.2764,\n",
      "        -2.2137, -3.9776, -0.6650,  0.8076,  1.8392,  3.3062, -1.6290,  0.2321,\n",
      "         1.1316,  1.3573,  0.2966, -3.9443,  3.7847,  1.7054, -3.2933,  0.9831,\n",
      "        -2.2024,  2.0837, -0.2746, -0.7497])\n",
      "w corrigé :  tensor([ 0.7121, -0.7300, -0.2501, -0.8275,  0.1612,  0.3884, -0.2028, -0.5354,\n",
      "        -0.3654,  0.5633,  0.1945,  0.8034,  0.0676,  1.7680, -2.2169,  0.7067,\n",
      "        -0.4836,  1.7853, -0.2207,  1.3784,  0.0781,  0.6871,  0.9736, -0.3032,\n",
      "        -0.5252,  1.4488,  0.4443,  0.8697, -0.7591, -0.8072,  0.4655, -2.3457,\n",
      "        -0.5366,  1.8044,  2.2803,  0.1670,  1.5337, -0.4929,  0.5970,  0.2076,\n",
      "         0.7678, -2.1280, -0.7029, -0.1666,  0.6189, -0.4965,  1.1158,  2.4281,\n",
      "         1.6990,  1.9451, -0.1456, -0.6157, -0.2249,  1.0188, -0.5957, -1.3808,\n",
      "         0.6160, -1.3886,  1.7203, -1.4838, -0.6836,  0.6456,  0.0944, -0.7313,\n",
      "         0.5081,  0.6103, -0.6151,  0.3251,  1.0263,  0.9054, -0.0550,  0.8853,\n",
      "        -0.1701,  1.2108,  0.5045, -0.4578, -1.4223,  0.4661,  0.3775,  0.6372,\n",
      "        -1.1058, -1.9878, -0.3315,  0.4028,  0.9186,  1.6521, -0.8135,  0.1150,\n",
      "         0.5648,  0.6776,  0.1473, -1.9712,  1.8914,  0.8517, -1.6457,  0.4905,\n",
      "        -1.1002,  1.0409, -0.1363, -0.3738], requires_grad=True)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(100,dtype=torch.float,requires_grad=True)\n",
    "b = torch.randn(1,dtype=torch.float,requires_grad=True)\n",
    "print(\"W init : \",w)\n",
    "## on optimise selon w et b.  lr est le pas du gradient\n",
    "optim = torch.optim.Adam(params=[w,b],lr=1e-3)\n",
    "auhasard = (w.dot(w)+b)\n",
    "#Mise à zéro du gradient\n",
    "optim.zero_grad()\n",
    "# Calcul du gradient\n",
    "auhasard.backward()\n",
    "print(\"gradient : \",w.grad)\n",
    "#Mise à jour des paramètres\n",
    "optim.step()\n",
    "print(\"w corrigé : \",w)\n",
    "#Mise à zéro du gradient\n",
    "optim.zero_grad()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ré-implémentez la fonction de descente de gradient en utilisant un optimiseur.\n",
    "\n",
    "L'optimiseur le plus utilisé  est **Adam** qui permet une pas adaptatif en fonction de l'historique. Sur ce problème simple vous ne verrez pas de différence, vous pourrez comparer par la suite **SGD** et **Adam**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6645,
     "status": "ok",
     "timestamp": 1700496829225,
     "user": {
      "displayName": "Lau re",
      "userId": "03302099944040145915"
     },
     "user_tz": -60
    },
    "id": "dftaEnaoIupc",
    "outputId": "5521979c-f770-4e67-e985-83ab12147322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W init :  tensor([-1.2921,  1.3073, -0.8787, -1.5463, -0.9669,  0.2395,  0.5530,  1.7907,\n",
      "         0.2371,  0.3826,  0.0784,  0.6989, -1.3610, -0.2072, -2.0893,  0.1799,\n",
      "         0.9368, -0.0691, -2.0463, -0.4978, -1.0464, -0.8335,  0.3908, -0.4537,\n",
      "        -0.4957, -0.7649,  2.1628, -0.1446,  1.9393, -0.5411, -0.9244, -0.1629,\n",
      "        -0.0926, -0.9837,  0.2582, -1.6644,  0.0053,  1.3103,  0.4799,  1.4335,\n",
      "        -0.1587,  1.3488,  0.0305,  0.8234,  1.3456,  1.3567,  0.6100, -0.3233,\n",
      "        -0.2143, -0.6587,  0.2943, -0.7358, -1.3086,  2.6678, -0.1649,  0.0133,\n",
      "         0.2268, -0.0492,  0.0384,  1.9174, -0.2334,  1.5561,  2.6947,  0.0601,\n",
      "        -0.2727, -0.1762, -0.8846,  0.9190,  0.5035,  0.9606,  0.9184, -1.0075,\n",
      "        -1.8775, -0.7746,  0.4312,  0.0332, -0.1825,  0.0948, -1.3293,  1.2419,\n",
      "        -0.0277,  0.6381, -0.3543,  0.1498,  0.6228,  2.2212, -0.0984,  1.2106,\n",
      "        -0.8217, -0.5655,  0.2568,  0.7904,  0.9377,  0.5259,  0.0582,  1.1714,\n",
      "         0.3629, -0.0495,  0.9534, -1.6239], requires_grad=True)\n",
      "gradient :  tensor([-2.5843,  2.6145, -1.7574, -3.0926, -1.9338,  0.4789,  1.1061,  3.5815,\n",
      "         0.4742,  0.7651,  0.1567,  1.3978, -2.7220, -0.4144, -4.1787,  0.3598,\n",
      "         1.8735, -0.1381, -4.0926, -0.9957, -2.0929, -1.6670,  0.7815, -0.9075,\n",
      "        -0.9915, -1.5297,  4.3256, -0.2893,  3.8786, -1.0822, -1.8488, -0.3259,\n",
      "        -0.1853, -1.9674,  0.5163, -3.3288,  0.0106,  2.6206,  0.9599,  2.8670,\n",
      "        -0.3175,  2.6976,  0.0610,  1.6468,  2.6912,  2.7134,  1.2200, -0.6466,\n",
      "        -0.4285, -1.3174,  0.5886, -1.4716, -2.6171,  5.3355, -0.3298,  0.0266,\n",
      "         0.4537, -0.0983,  0.0769,  3.8348, -0.4668,  3.1122,  5.3894,  0.1202,\n",
      "        -0.5455, -0.3524, -1.7693,  1.8381,  1.0071,  1.9212,  1.8369, -2.0149,\n",
      "        -3.7549, -1.5492,  0.8625,  0.0665, -0.3650,  0.1895, -2.6585,  2.4838,\n",
      "        -0.0553,  1.2762, -0.7085,  0.2995,  1.2456,  4.4423, -0.1969,  2.4213,\n",
      "        -1.6433, -1.1311,  0.5136,  1.5807,  1.8754,  1.0518,  0.1163,  2.3429,\n",
      "         0.7258, -0.0990,  1.9068, -3.2478])\n",
      "w corrigé :  tensor([-1.2896,  1.3046, -0.8769, -1.5432, -0.9649,  0.2390,  0.5519,  1.7871,\n",
      "         0.2366,  0.3818,  0.0782,  0.6975, -1.3583, -0.2068, -2.0852,  0.1796,\n",
      "         0.9349, -0.0689, -2.0422, -0.4968, -1.0443, -0.8318,  0.3900, -0.4528,\n",
      "        -0.4947, -0.7633,  2.1585, -0.1443,  1.9354, -0.5400, -0.9226, -0.1626,\n",
      "        -0.0924, -0.9818,  0.2576, -1.6611,  0.0053,  1.3077,  0.4790,  1.4307,\n",
      "        -0.1584,  1.3461,  0.0305,  0.8218,  1.3429,  1.3540,  0.6088, -0.3226,\n",
      "        -0.2138, -0.6574,  0.2937, -0.7343, -1.3059,  2.6624, -0.1646,  0.0133,\n",
      "         0.2264, -0.0491,  0.0384,  1.9136, -0.2329,  1.5530,  2.6893,  0.0600,\n",
      "        -0.2722, -0.1759, -0.8829,  0.9172,  0.5025,  0.9587,  0.9166, -1.0055,\n",
      "        -1.8737, -0.7731,  0.4304,  0.0332, -0.1821,  0.0946, -1.3266,  1.2394,\n",
      "        -0.0276,  0.6368, -0.3535,  0.1495,  0.6215,  2.2167, -0.0982,  1.2082,\n",
      "        -0.8200, -0.5644,  0.2563,  0.7888,  0.9358,  0.5249,  0.0580,  1.1691,\n",
      "         0.3622, -0.0494,  0.9515, -1.6206], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(100,dtype=torch.float,requires_grad=True)\n",
    "b = torch.randn(1,dtype=torch.float,requires_grad=True)\n",
    "print(\"W init : \",w)\n",
    "## on optimise selon w et b.  lr est le pas du gradient\n",
    "optim = torch.optim.SGD(params=[w,b],lr=1e-3)\n",
    "auhasard = (w.dot(w)+b)\n",
    "#Mise à zéro du gradient\n",
    "optim.zero_grad()\n",
    "# Calcul du gradient\n",
    "auhasard.backward()\n",
    "print(\"gradient : \",w.grad)\n",
    "#Mise à jour des paramètres\n",
    "optim.step()\n",
    "print(\"w corrigé : \",w)\n",
    "#Mise à zéro du gradient\n",
    "optim.zero_grad()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHY1ffLNJxsb"
   },
   "source": [
    "\n",
    "## DataLoader\n",
    "Le <a href=https://pytorch.org/docs/stable/data.html>**DataLoader**</a> et la classe associée <a href=https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset> **Dataset**</a>  permettent en particulier de :\n",
    "* charger des données\n",
    "* pré-processer les données\n",
    "* de gérer les mini-batchs (sous-ensembles sur lequel on effectue une descente de gradient).\n",
    "\n",
    "La classe **Dataset** est une classe abstraite qui nécessite l'implémentation que d'une seule méthode, ```__getitem__(self,index)``` : elle renvoie le i-ème objet du jeu de données (généralement un couple *(exemple,label)*.\n",
    "\n",
    "La classe **TensorDataset** est l'instanciation la plus courante d'un **Dataset**, elle permet de créer un objet **Dataset** à partir d'une liste de tenseurs qui renvoie pour un index $i$ donné le tuple contenant les $i$-èmes ligne de chaque tenseur.\n",
    "\n",
    "La classe **DataLoader** permet essentiellement de randomiser et de constituer des mini-batchs de façon simple à partir d'une instance de **Dataset**. Chaque mini-batch est constitué d'exemples tirés aléatoirement dans le **Dataset** passé en paramètre et mis bout à bout dans des tenseurs. La méthode ```collate_fn(*args)``` est utilisée pour cela (nous ne l'utiliserons pas pour l'instant). C'est ce générateur qui est généralement parcouru lors de l'apprentissage à chaque itération d'optimisation.\n",
    "\n",
    "Voici un exemple de code pour utiliser le DataLoader :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hg-ziayDJsbS"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "## Création d'un dataset à partir des deux tenseurs d'exemples et de labels\n",
    "train_data = TensorDataset(housing_x,housing_y)\n",
    "\n",
    "## On peut indexer et connaitre la longueur d'un dataset\n",
    "print(len(train_data),train_data[5])\n",
    "\n",
    "## Création d'un DataLoader\n",
    "## tailles de mini-batch de 16, shuffle=True permet de mélanger les exemples\n",
    "# loader est un itérateur sur les mini-batchs des données\n",
    "loader = DataLoader(train_data, batch_size=16,shuffle=True )\n",
    "\n",
    "#Premier batch (aléatoire) du dataloader :\n",
    "print(len(iter(loader)),next(iter(loader)))\n",
    "for x,y in loader:\n",
    "    print(x,y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifiez votre code pour utiliser un Dataloader : **descente_gradient(loader, loss, eps, epochs) **.\n",
    "\n",
    "Oubliez pas qu'une époque correspond à la prise en compte de tous les exemples une fois. Le coût n'est généralement enregistré qu'à la fin d'une époque, cumulé sur tous les batchs. Vous pouvez utiliser la fonction **item()** d'un tenseur pour avoir accès au scalaire sans garder de trace du graphe de calcul.\n",
    "\n",
    "Testez et observez les différences en fonction de la taille des mini-batchs. Est-ce plus rapide ou plus lent que la version batch ? pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKs1Q5pPKf-2"
   },
   "source": [
    "\n",
    "## TensorBoard\n",
    "\n",
    "Durant l'apprentissage de vos modèles, il est agréable de visualiser de quelle manière évolue le coût, la précision sur l'ensemble de validation ainsi que d'autres éléments. TensorFlow dispose d'un outil très apprécié, le TensorBoard, qui permet de gérer très facilement de tels affichages. On retrouve tensorboard dans **Pytorch** dans ```torch.utils.ensorboard``` qui permet de faire le pont de pytorch vers cet outil.\n",
    "\n",
    "Le principe est le suivant :\n",
    "* tensorboard fait tourner en fait un serveur web local qui va lire les fichiers de log dans un répertoire local. L'affichage se fait dans votre navigateur à partir d'un lien fourni lors du lancement de tensorboard.\n",
    "* Les éléments que vous souhaitez visualiser (scalaire, graphes, distributions, histogrammes) sont écrits dans le fichier de log à partir d'un objet **SummaryWriter** .\n",
    "* la méthode ```add_scalar(tag, valeur, global_step)``` permet de logger une valeur à un step donné, ```add_scalar(tag, tag_scalar_dic, global_step)``` un ensemble de valeurs par l'intermédiaire du dictionnaire ```tag_scalar_dic``` (un regroupement des scalaires est fait en fonction du tag passé, chaque sous-tag séparé par un **/**).\n",
    "\n",
    "Il existe d'autres méthodes ```add_XXX``` pour visualiser par exemple des images, des histogrammes (cf <a href=https://pytorch.org/docs/stable/tensorboard.html>la doc </a>).\n",
    "\n",
    "Le code suivant illustre une manière de l'utiliser.\n",
    "Pour information, il est bien plus agréable de l'utiliser en dehors d'un notebook que à l'intérieur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG9P8sKPKdcq"
   },
   "outputs": [],
   "source": [
    "#Spécial notebook, les commandes suivantes permettent de lancer tensorboard\n",
    "# En dehors du notebook, il faut le lancer à la main dans le shell :\n",
    "# tensorboard --logdir logs\n",
    "TB_PATH = \"/tmp/logs/module1\"\n",
    "import os\n",
    "os.makedirs(TB_PATH,exist_ok=True)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {TB_PATH}\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Spécial notebook : pour avoir les courbes qui s'affichent dans le notebook,\n",
    "# sinon aller à l'adresse web local indiquée lors du lancement de tensorboard\n",
    "from tensorboard import notebook\n",
    "notebook.display()\n",
    "# Où seront stockés les logs.\n",
    "summary = SummaryWriter(f\"{TB_PATH}/test\")\n",
    "\n",
    "mseloss = torch.nn.MSELoss()\n",
    "for e in range(1000):\n",
    "    summary.add_scalar(\"loss/train\",torch.randn(1,1).item(),e)\n",
    "    summary.add_scalar(\"loss/test\",torch.randn(1,1).item(),e)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intégrez Tensorboard à votre code et testez le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
